{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2404335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\file\\pettingzoo_rl_practice\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "F:\\file\\pettingzoo_rl_practice\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:2: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils.version import LooseVersion\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "agent_iter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     32\u001b[0m     totalreward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_iter\u001b[49m():\n\u001b[0;32m     34\u001b[0m         obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mlast()\n\u001b[0;32m     35\u001b[0m         totalreward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mF:\\file\\pettingzoo_rl_practice\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:312\u001b[0m, in \u001b[0;36mVecEnvWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Recursive attribute lookup for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mown_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mambiguous and hides attribute from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblocked_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(error_str)\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetattr_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\file\\pettingzoo_rl_practice\\lib\\site-packages\\supersuit\\vector\\sb3_vector_wrapper.py:24\u001b[0m, in \u001b[0;36mSB3VecEnvWrapper.getattr_recursive\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetattr_recursive\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: agent_iter"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3 import PPO\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = pistonball_v6.parallel_env(n_pistons=20,\n",
    "                                 time_penalty=-0.1,\n",
    "                                 continuous=True,\n",
    "                                 random_drop=True,\n",
    "                                 random_rotate=True,\n",
    "                                 ball_mass=0.75,\n",
    "                                 ball_friction=0.3,\n",
    "                                 ball_elasticity=1.5,\n",
    "                                 max_cycles=125\n",
    "                                )\n",
    "\n",
    "env = ss.color_reduction_v0(env, mode= 'B')\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')\n",
    "\n",
    "done = 0\n",
    "records1 = []\n",
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    env.reset()\n",
    "    done =0\n",
    "    while not done:\n",
    "        totalreward = 0\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, done, info = env.last()\n",
    "            totalreward += reward\n",
    "            act = env.action_space(agent).sample()\n",
    "            env.step(act)\n",
    "    records1.append(totalreward)\n",
    "    print('totalreward:',totalreward,'\\n')\n",
    "plt.plot(records1)\n",
    "plt.show()\n",
    "\n",
    "model = PPO(CnnPolicy,\n",
    "            env,\n",
    "            verbose=3,\n",
    "            gamma=0.95,\n",
    "            n_steps=256,\n",
    "            ent_coef=0.0905168,\n",
    "            learning_rate=0.00062211,\n",
    "            vf_coef=0.042202,\n",
    "            max_grad_norm=0.9,\n",
    "            gae_lambda=0.99,\n",
    "            n_epochs=5,\n",
    "            clip_range=0.3,\n",
    "            batch_size=256\n",
    "           )\n",
    "model.learn(total_timesteps=2000000)\n",
    "model.save('pp')\n",
    "\n",
    "\n",
    "\n",
    "env = pistonball_v6.env()\n",
    "env = ss.color_reduction_v0(env, mode='B')\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "model = PPO.load('pp')\n",
    "done = 0\n",
    "records2 = []\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    env.reset()\n",
    "    done =0\n",
    "    while not done:\n",
    "        totalreward = 0\n",
    "        for agent in env.agent_iter():\n",
    "            #env.render()\n",
    "            obs, reward, done, info = env.last()\n",
    "            totalreward += reward\n",
    "            act = model.predict(obs, deterministic=True)[0] if not done else None\n",
    "            env.step(act)\n",
    "    records2.append(totalreward)\n",
    "    print('totalreward:',totalreward,'\\n')\n",
    "plt.plot(records2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69de5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 803   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 360         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012186225 |\n",
      "|    clip_fraction        | 0.0395      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.0131      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.32        |\n",
      "|    n_updates            | 5           |\n",
      "|    policy_gradient_loss | 0.00294     |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 9.13        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 293         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012686551 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | 0.0139      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.00295     |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 6.06        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 265         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008333415 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.00908     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.137       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | 0.00245     |\n",
      "|    std                  | 1.38        |\n",
      "|    value_loss           | 7.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 827         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008411718 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.00613     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0026      |\n",
      "|    std                  | 1.52        |\n",
      "|    value_loss           | 11.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1023        |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009965043 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 0.00106     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.0851      |\n",
      "|    n_updates            | 25          |\n",
      "|    policy_gradient_loss | 0.0024      |\n",
      "|    std                  | 1.66        |\n",
      "|    value_loss           | 8.84        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1223        |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009028109 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2          |\n",
      "|    explained_variance   | -0.0193     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00343     |\n",
      "|    std                  | 1.8         |\n",
      "|    value_loss           | 16.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1427        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011906453 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.08       |\n",
      "|    explained_variance   | 0.00416     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.664       |\n",
      "|    n_updates            | 35          |\n",
      "|    policy_gradient_loss | 0.000821    |\n",
      "|    std                  | 1.96        |\n",
      "|    value_loss           | 20.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1620        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009377211 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 0.00807     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.432       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.000912    |\n",
      "|    std                  | 2.12        |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1821        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009229581 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.22       |\n",
      "|    explained_variance   | -0.00198    |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 0.845       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.00113     |\n",
      "|    std                  | 2.29        |\n",
      "|    value_loss           | 33.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 2016        |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008969257 |\n",
      "|    clip_fraction        | 0.0385      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.31       |\n",
      "|    explained_variance   | -0.156      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00195     |\n",
      "|    std                  | 2.48        |\n",
      "|    value_loss           | 50.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 2233        |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008593191 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | -0.0266     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 55          |\n",
      "|    policy_gradient_loss | 0.00142     |\n",
      "|    std                  | 2.67        |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 2454        |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008443299 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.0143      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00138     |\n",
      "|    std                  | 2.88        |\n",
      "|    value_loss           | 61.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 2675        |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009006898 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | -0.00551    |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 65          |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    std                  | 3.12        |\n",
      "|    value_loss           | 51.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 2893        |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009302207 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.0344      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.79        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00212     |\n",
      "|    std                  | 3.37        |\n",
      "|    value_loss           | 78          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 3110        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009503578 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.0221      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.00183     |\n",
      "|    std                  | 3.65        |\n",
      "|    value_loss           | 71.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 3328        |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012793821 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | -0.00979    |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.57        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00151     |\n",
      "|    std                  | 4           |\n",
      "|    value_loss           | 72.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 3533        |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009353582 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0112      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 85          |\n",
      "|    policy_gradient_loss | 0.000673    |\n",
      "|    std                  | 4.29        |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 3729        |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008113482 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.00896     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00207     |\n",
      "|    std                  | 4.62        |\n",
      "|    value_loss           | 47.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 3912        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010552896 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.02       |\n",
      "|    explained_variance   | 0.0255      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 95          |\n",
      "|    policy_gradient_loss | 0.00183     |\n",
      "|    std                  | 5.06        |\n",
      "|    value_loss           | 62.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 4097        |\n",
      "|    total_timesteps      | 860160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010819514 |\n",
      "|    clip_fraction        | 0.043       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.11       |\n",
      "|    explained_variance   | 0.0362      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.32        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00147     |\n",
      "|    std                  | 5.51        |\n",
      "|    value_loss           | 68.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 4284        |\n",
      "|    total_timesteps      | 901120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008599292 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.18       |\n",
      "|    explained_variance   | 0.0308      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.5         |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    std                  | 5.95        |\n",
      "|    value_loss           | 67.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 4470        |\n",
      "|    total_timesteps      | 942080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009169035 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.27       |\n",
      "|    explained_variance   | 0.0101      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 3.08        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00226     |\n",
      "|    std                  | 6.44        |\n",
      "|    value_loss           | 76.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 4655        |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008553648 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.34       |\n",
      "|    explained_variance   | 0.014       |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 115         |\n",
      "|    policy_gradient_loss | 0.00166     |\n",
      "|    std                  | 6.96        |\n",
      "|    value_loss           | 66.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 4841        |\n",
      "|    total_timesteps      | 1024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007992933 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.42       |\n",
      "|    explained_variance   | 0.0156      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.63        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.00206     |\n",
      "|    std                  | 7.52        |\n",
      "|    value_loss           | 69.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 5026         |\n",
      "|    total_timesteps      | 1064960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085420925 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -3.5         |\n",
      "|    explained_variance   | 0.0152       |\n",
      "|    learning_rate        | 0.000622     |\n",
      "|    loss                 | 2.19         |\n",
      "|    n_updates            | 125          |\n",
      "|    policy_gradient_loss | 0.00146      |\n",
      "|    std                  | 8.16         |\n",
      "|    value_loss           | 62.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 5213        |\n",
      "|    total_timesteps      | 1105920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009770136 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.58       |\n",
      "|    explained_variance   | 0.0344      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 3.47        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00187     |\n",
      "|    std                  | 8.89        |\n",
      "|    value_loss           | 80.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 5398        |\n",
      "|    total_timesteps      | 1146880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008196987 |\n",
      "|    clip_fraction        | 0.0313      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.66       |\n",
      "|    explained_variance   | 0.00297     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.3         |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | 0.00159     |\n",
      "|    std                  | 9.59        |\n",
      "|    value_loss           | 74.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 5583        |\n",
      "|    total_timesteps      | 1187840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008968836 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.75       |\n",
      "|    explained_variance   | 0.00081     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 3.82        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.00131     |\n",
      "|    std                  | 10.4        |\n",
      "|    value_loss           | 92.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 5768        |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008092162 |\n",
      "|    clip_fraction        | 0.0301      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.82       |\n",
      "|    explained_variance   | 0.0135      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.4         |\n",
      "|    n_updates            | 145         |\n",
      "|    policy_gradient_loss | 0.0011      |\n",
      "|    std                  | 11.2        |\n",
      "|    value_loss           | 71.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 5953        |\n",
      "|    total_timesteps      | 1269760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008743336 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -3.9        |\n",
      "|    explained_variance   | 0.000611    |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 3.04        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00176     |\n",
      "|    std                  | 12.1        |\n",
      "|    value_loss           | 74.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 6141       |\n",
      "|    total_timesteps      | 1310720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00952559 |\n",
      "|    clip_fraction        | 0.0371     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -3.98      |\n",
      "|    explained_variance   | 0.00917    |\n",
      "|    learning_rate        | 0.000622   |\n",
      "|    loss                 | 2.59       |\n",
      "|    n_updates            | 155        |\n",
      "|    policy_gradient_loss | 0.00137    |\n",
      "|    std                  | 13.2       |\n",
      "|    value_loss           | 75.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 6325        |\n",
      "|    total_timesteps      | 1351680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009267338 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.07       |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.55        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.00207     |\n",
      "|    std                  | 14.3        |\n",
      "|    value_loss           | 76.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 6512        |\n",
      "|    total_timesteps      | 1392640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009268234 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.15       |\n",
      "|    explained_variance   | -0.00937    |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    std                  | 15.6        |\n",
      "|    value_loss           | 49.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 6697        |\n",
      "|    total_timesteps      | 1433600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007940731 |\n",
      "|    clip_fraction        | 0.0306      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.23       |\n",
      "|    explained_variance   | 0.0196      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.00116     |\n",
      "|    std                  | 16.8        |\n",
      "|    value_loss           | 53.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 6883        |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009309897 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.31       |\n",
      "|    explained_variance   | -0.0144     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 175         |\n",
      "|    policy_gradient_loss | 0.00178     |\n",
      "|    std                  | 18.3        |\n",
      "|    value_loss           | 59.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 7072        |\n",
      "|    total_timesteps      | 1515520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008964746 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.4        |\n",
      "|    explained_variance   | -0.0388     |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00222     |\n",
      "|    std                  | 19.9        |\n",
      "|    value_loss           | 44.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 7266        |\n",
      "|    total_timesteps      | 1556480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006790967 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.0499      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 185         |\n",
      "|    policy_gradient_loss | 0.00167     |\n",
      "|    std                  | 21.5        |\n",
      "|    value_loss           | 56.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 7451        |\n",
      "|    total_timesteps      | 1597440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008132213 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.55       |\n",
      "|    explained_variance   | 0.0107      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.00229     |\n",
      "|    std                  | 23.2        |\n",
      "|    value_loss           | 47.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 7638        |\n",
      "|    total_timesteps      | 1638400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008225316 |\n",
      "|    clip_fraction        | 0.0306      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.63       |\n",
      "|    explained_variance   | 0.032       |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 195         |\n",
      "|    policy_gradient_loss | 0.00188     |\n",
      "|    std                  | 25.2        |\n",
      "|    value_loss           | 57.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 7839        |\n",
      "|    total_timesteps      | 1679360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008164714 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.72       |\n",
      "|    explained_variance   | 0.029       |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | 0.00187     |\n",
      "|    std                  | 27.4        |\n",
      "|    value_loss           | 43.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 8044        |\n",
      "|    total_timesteps      | 1720320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008283588 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.0489      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 205         |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    std                  | 29.7        |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 8247        |\n",
      "|    total_timesteps      | 1761280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007771445 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.87       |\n",
      "|    explained_variance   | -0.000157   |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.00288     |\n",
      "|    std                  | 32.2        |\n",
      "|    value_loss           | 49.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 8444        |\n",
      "|    total_timesteps      | 1802240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007053499 |\n",
      "|    clip_fraction        | 0.0281      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.0434      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 215         |\n",
      "|    policy_gradient_loss | 0.00185     |\n",
      "|    std                  | 34.4        |\n",
      "|    value_loss           | 55          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 8642        |\n",
      "|    total_timesteps      | 1843200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008710417 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -5.03       |\n",
      "|    explained_variance   | 0.0141      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.00197     |\n",
      "|    std                  | 37.4        |\n",
      "|    value_loss           | 54.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 8842         |\n",
      "|    total_timesteps      | 1884160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077630132 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -5.1         |\n",
      "|    explained_variance   | 0.034        |\n",
      "|    learning_rate        | 0.000622     |\n",
      "|    loss                 | 1.74         |\n",
      "|    n_updates            | 225          |\n",
      "|    policy_gradient_loss | 0.00174      |\n",
      "|    std                  | 40.3         |\n",
      "|    value_loss           | 54.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 9041        |\n",
      "|    total_timesteps      | 1925120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007796067 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -5.17       |\n",
      "|    explained_variance   | 0.0187      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.000912    |\n",
      "|    std                  | 43.1        |\n",
      "|    value_loss           | 54.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 9245        |\n",
      "|    total_timesteps      | 1966080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008571277 |\n",
      "|    clip_fraction        | 0.0319      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -5.24       |\n",
      "|    explained_variance   | 0.0315      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 235         |\n",
      "|    policy_gradient_loss | 0.00188     |\n",
      "|    std                  | 46.6        |\n",
      "|    value_loss           | 52          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 9451        |\n",
      "|    total_timesteps      | 2007040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007757169 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -5.33       |\n",
      "|    explained_variance   | 0.0606      |\n",
      "|    learning_rate        | 0.000622    |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    std                  | 50.3        |\n",
      "|    value_loss           | 49.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\file\\pettingzoo_rl_practice\\lib\\site-packages\\gym\\utils\\seeding.py:63: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env(\n",
    "    n_pistons=20,\n",
    "    time_penalty=-0.1,\n",
    "    continuous=True,\n",
    "    random_drop=True,\n",
    "    random_rotate=True,\n",
    "    ball_mass=0.75,\n",
    "    ball_friction=0.3,\n",
    "    ball_elasticity=1.5,\n",
    "    max_cycles=125,\n",
    ")\n",
    "env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class=\"stable_baselines3\")\n",
    "model = PPO(\n",
    "    CnnPolicy,\n",
    "    env,\n",
    "    verbose=3,\n",
    "    gamma=0.95,\n",
    "    n_steps=256,\n",
    "    ent_coef=0.0905168,\n",
    "    learning_rate=0.00062211,\n",
    "    vf_coef=0.042202,\n",
    "    max_grad_norm=0.9,\n",
    "    gae_lambda=0.99,\n",
    "    n_epochs=5,\n",
    "    clip_range=0.3,\n",
    "    batch_size=256,\n",
    ")\n",
    "model.learn(total_timesteps=2000000)\n",
    "model.save(\"policy\")\n",
    "\n",
    "# Rendering\n",
    "\n",
    "env = pistonball_v6.env()\n",
    "env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "model = PPO.load(\"policy\")\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    obs, reward, done, info = env.last()\n",
    "    act = model.predict(obs, deterministic=True)[0] if not done else None\n",
    "    env.step(act)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training,this is the 0 th episode,with the total reward: 1964.0000000000027\n",
      "After training,this is the 1 th episode,with the total reward: 1943.9999999999907\n",
      "After training,this is the 2 th episode,with the total reward: 1959.9999999999957\n",
      "After training,this is the 3 th episode,with the total reward: 1966.000000000001\n",
      "After training,this is the 4 th episode,with the total reward: 1963.9999999999945\n",
      "After training,this is the 5 th episode,with the total reward: 1958.0000000000034\n",
      "After training,this is the 6 th episode,with the total reward: 1960.0000000000064\n",
      "After training,this is the 7 th episode,with the total reward: 1955.999999999999\n",
      "After training,this is the 8 th episode,with the total reward: 1969.9999999999895\n",
      "After training,this is the 9 th episode,with the total reward: 1962.0000000000118\n",
      "After training,this is the 10 th episode,with the total reward: 1944.0000000000007\n",
      "After training,this is the 11 th episode,with the total reward: 1949.9999999999948\n",
      "After training,this is the 12 th episode,with the total reward: 1956.0000000000048\n",
      "After training,this is the 13 th episode,with the total reward: 1945.9999999999995\n",
      "After training,this is the 14 th episode,with the total reward: 1965.9999999999995\n",
      "After training,this is the 15 th episode,with the total reward: 1945.999999999996\n",
      "After training,this is the 16 th episode,with the total reward: 1969.9999999999932\n",
      "After training,this is the 17 th episode,with the total reward: 1950.0000000000018\n",
      "After training,this is the 18 th episode,with the total reward: 1960.0000000000039\n",
      "After training,this is the 19 th episode,with the total reward: 1954.244604316545\n",
      "After training,this is the 20 th episode,with the total reward: 1951.9999999999984\n",
      "After training,this is the 21 th episode,with the total reward: 1964.0000000000025\n",
      "After training,this is the 22 th episode,with the total reward: 1948.0000000000007\n",
      "After training,this is the 23 th episode,with the total reward: 1933.9999999999982\n",
      "After training,this is the 24 th episode,with the total reward: 1951.3916786226755\n",
      "After training,this is the 25 th episode,with the total reward: 1945.9999999999995\n",
      "After training,this is the 26 th episode,with the total reward: 1969.9999999999998\n",
      "After training,this is the 27 th episode,with the total reward: 1966.0000000000039\n",
      "After training,this is the 28 th episode,with the total reward: 1952.0000000000005\n",
      "After training,this is the 29 th episode,with the total reward: 1968.0000000000002\n",
      "After training,this is the 30 th episode,with the total reward: 1967.9999999999932\n",
      "After training,this is the 31 th episode,with the total reward: 1952.000000000001\n",
      "After training,this is the 32 th episode,with the total reward: 1963.9999999999982\n",
      "After training,this is the 33 th episode,with the total reward: 1935.1949509116325\n",
      "After training,this is the 34 th episode,with the total reward: 1960.0000000000045\n",
      "After training,this is the 35 th episode,with the total reward: 1965.9999999999918\n",
      "After training,this is the 36 th episode,with the total reward: 1967.9999999999986\n",
      "After training,this is the 37 th episode,with the total reward: 1961.999999999994\n",
      "After training,this is the 38 th episode,with the total reward: 1964.000000000006\n",
      "After training,this is the 39 th episode,with the total reward: 1947.9999999999948\n",
      "After training,this is the 40 th episode,with the total reward: 1964.0000000000007\n",
      "After training,this is the 41 th episode,with the total reward: 1966.0000000000018\n",
      "After training,this is the 42 th episode,with the total reward: 1963.999999999996\n",
      "After training,this is the 43 th episode,with the total reward: 1972.0000000000002\n",
      "After training,this is the 44 th episode,with the total reward: 1948.0000000000048\n",
      "After training,this is the 45 th episode,with the total reward: 1958.0000000000023\n",
      "After training,this is the 46 th episode,with the total reward: 1961.9999999999993\n",
      "After training,this is the 47 th episode,with the total reward: 1966.0000000000073\n",
      "After training,this is the 48 th episode,with the total reward: 1940.0000000000005\n",
      "After training,this is the 49 th episode,with the total reward: 1963.999999999999\n",
      "After training,this is the 50 th episode,with the total reward: 1958.0000000000057\n",
      "After training,this is the 51 th episode,with the total reward: 1933.9999999999961\n",
      "After training,this is the 52 th episode,with the total reward: 1961.9999999999993\n",
      "After training,this is the 53 th episode,with the total reward: 1976.0000000000034\n",
      "After training,this is the 54 th episode,with the total reward: 1961.999999999999\n",
      "After training,this is the 55 th episode,with the total reward: 1946.0000000000039\n",
      "After training,this is the 56 th episode,with the total reward: 1955.9999999999989\n",
      "After training,this is the 57 th episode,with the total reward: 1950.0000000000057\n",
      "After training,this is the 58 th episode,with the total reward: 1936.0000000000045\n",
      "After training,this is the 59 th episode,with the total reward: 1963.9999999999973\n",
      "After training,this is the 60 th episode,with the total reward: 1944.0000000000002\n",
      "After training,this is the 61 th episode,with the total reward: 1944.0000000000034\n",
      "After training,this is the 62 th episode,with the total reward: 1967.9999999999989\n",
      "After training,this is the 63 th episode,with the total reward: 1964.0000000000007\n",
      "After training,this is the 64 th episode,with the total reward: 1956.000000000004\n",
      "After training,this is the 65 th episode,with the total reward: 1959.9999999999927\n",
      "After training,this is the 66 th episode,with the total reward: 1963.9999999999955\n",
      "After training,this is the 67 th episode,with the total reward: 1964.0000000000057\n",
      "After training,this is the 68 th episode,with the total reward: 1947.9999999999895\n",
      "After training,this is the 69 th episode,with the total reward: 1959.9999999999995\n",
      "After training,this is the 70 th episode,with the total reward: 1962.000000000006\n",
      "After training,this is the 71 th episode,with the total reward: 1973.9999999999934\n",
      "After training,this is the 72 th episode,with the total reward: 1955.9999999999932\n",
      "After training,this is the 73 th episode,with the total reward: 1930.0000000000127\n",
      "After training,this is the 74 th episode,with the total reward: 1935.9999999999875\n",
      "After training,this is the 75 th episode,with the total reward: 1956.000000000002\n",
      "After training,this is the 76 th episode,with the total reward: 1953.9999999999995\n",
      "After training,this is the 77 th episode,with the total reward: 1946.0000000000082\n",
      "After training,this is the 78 th episode,with the total reward: 1954.0000000000005\n",
      "After training,this is the 79 th episode,with the total reward: 1962.0000000000039\n",
      "After training,this is the 80 th episode,with the total reward: 1962.0\n",
      "After training,this is the 81 th episode,with the total reward: 1963.999999999992\n",
      "After training,this is the 82 th episode,with the total reward: 1963.9999999999957\n",
      "After training,this is the 83 th episode,with the total reward: 1956.0000000000036\n",
      "After training,this is the 84 th episode,with the total reward: 1956.0000000000011\n",
      "After training,this is the 85 th episode,with the total reward: 1959.9999999999968\n",
      "After training,this is the 86 th episode,with the total reward: 1960.0\n",
      "After training,this is the 87 th episode,with the total reward: 1960.0\n",
      "After training,this is the 88 th episode,with the total reward: 1933.999999999993\n",
      "After training,this is the 89 th episode,with the total reward: 1960.0000000000066\n",
      "After training,this is the 90 th episode,with the total reward: 1957.9999999999982\n",
      "After training,this is the 91 th episode,with the total reward: 1947.999999999993\n",
      "After training,this is the 92 th episode,with the total reward: 1946.0\n",
      "After training,this is the 93 th episode,with the total reward: 1941.9999999999993\n",
      "After training,this is the 94 th episode,with the total reward: 1947.999999999997\n",
      "After training,this is the 95 th episode,with the total reward: 1961.1949509116407\n",
      "After training,this is the 96 th episode,with the total reward: 1958.0000000000039\n",
      "After training,this is the 97 th episode,with the total reward: 1960.1434846266438\n",
      "After training,this is the 98 th episode,with the total reward: 1953.999999999999\n",
      "After training,this is the 99 th episode,with the total reward: 1972.0000000000018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training,this is the 0 th episode,with the total reward: -295.51920341394066\n",
      "Before training,this is the 1 th episode,with the total reward: -33.40365682137587\n",
      "Before training,this is the 2 th episode,with the total reward: -364.7058823529562\n",
      "Before training,this is the 3 th episode,with the total reward: 144.44444444444596\n",
      "Before training,this is the 4 th episode,with the total reward: 283.5195530726197\n",
      "Before training,this is the 5 th episode,with the total reward: 85.329341317364\n",
      "Before training,this is the 6 th episode,with the total reward: 548.825256975005\n",
      "Before training,this is the 7 th episode,with the total reward: -252.78551532032748\n",
      "Before training,this is the 8 th episode,with the total reward: -230.36465638148292\n",
      "Before training,this is the 9 th episode,with the total reward: -182.84671532845476\n",
      "Before training,this is the 10 th episode,with the total reward: 522.4550898203399\n",
      "Before training,this is the 11 th episode,with the total reward: -124.99999999999507\n",
      "Before training,this is the 12 th episode,with the total reward: 240.16641452344976\n",
      "Before training,this is the 13 th episode,with the total reward: -151.26939351198405\n",
      "Before training,this is the 14 th episode,with the total reward: 98.5967503692772\n",
      "Before training,this is the 15 th episode,with the total reward: 110.46511627906608\n",
      "Before training,this is the 16 th episode,with the total reward: -264.992503748127\n",
      "Before training,this is the 17 th episode,with the total reward: -48.83381924198271\n",
      "Before training,this is the 18 th episode,with the total reward: -169.1073919107307\n",
      "Before training,this is the 19 th episode,with the total reward: -280.1204819277105\n",
      "Before training,this is the 20 th episode,with the total reward: -322.0461095100913\n",
      "Before training,this is the 21 th episode,with the total reward: -347.9228486646884\n",
      "Before training,this is the 22 th episode,with the total reward: -367.82032400590157\n",
      "Before training,this is the 23 th episode,with the total reward: -396.26865671644146\n",
      "Before training,this is the 24 th episode,with the total reward: 93.61233480175447\n",
      "Before training,this is the 25 th episode,with the total reward: -289.7163120567386\n",
      "Before training,this is the 26 th episode,with the total reward: -124.47698744769473\n",
      "Before training,this is the 27 th episode,with the total reward: -304.3806646525777\n",
      "Before training,this is the 28 th episode,with the total reward: -208.10055865921308\n",
      "Before training,this is the 29 th episode,with the total reward: -281.0734463276788\n",
      "Before training,this is the 30 th episode,with the total reward: -275.89928057553345\n",
      "Before training,this is the 31 th episode,with the total reward: -321.74887892377313\n",
      "Before training,this is the 32 th episode,with the total reward: -139.6952104499219\n",
      "Before training,this is the 33 th episode,with the total reward: -151.54711673698563\n",
      "Before training,this is the 34 th episode,with the total reward: 191.95804195804268\n",
      "Before training,this is the 35 th episode,with the total reward: -267.3661360347231\n",
      "Before training,this is the 36 th episode,with the total reward: -295.51920341393947\n"
     ]
    }
   ],
   "source": [
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.parallel_env(\n",
    "    n_pistons=20,\n",
    "    time_penalty=-0.1,\n",
    "    continuous=True,\n",
    "    random_drop=True,\n",
    "    random_rotate=True,\n",
    "    ball_mass=0.75,\n",
    "    ball_friction=0.3,\n",
    "    ball_elasticity=1.5,\n",
    "    max_cycles=125,\n",
    ")\n",
    "env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class=\"stable_baselines3\")\n",
    "\n",
    "done = 0\n",
    "\n",
    "records2 = []\n",
    "env.reset()\n",
    "model = PPO.load(\"policy\")\n",
    "env = pistonball_v6.env()\n",
    "env = ss.color_reduction_v0(env, mode='B')\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "for i in range(100):\n",
    "    env.reset()\n",
    "    total = 0\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, done, info = env.last()\n",
    "        act = model.predict(obs, deterministic=True)[0] if not done else None\n",
    "        env.step(act)\n",
    "        total += reward\n",
    "    records2.append(total)\n",
    "    print('After training,this is the',i,'th episode,with the total reward:',total)\n",
    "        \n",
    "done = 0\n",
    "records1 = []\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    env.reset()\n",
    "    total = 0\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, done, info = env.last()\n",
    "        act = env.action_space(agent).sample()\n",
    "        env.step(act)\n",
    "        total += reward\n",
    "    records1.append(total)\n",
    "    print('Before training,this is the',i,'th episode,with the total reward:',total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556ce21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpettingzoo",
   "language": "python",
   "name": "venvpettingzoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
